{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de26510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp chat_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d28c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "from fastapi import APIRouter\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    AIMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from llama_index import (\n",
    "    GPTSimpleVectorIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    LLMPredictor,\n",
    "    ServiceContext,\n",
    ")\n",
    "from llama_index.readers.schema.base import Document\n",
    "from llama_index.prompts.chat_prompts import CHAT_REFINE_PROMPT\n",
    "from llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f728ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "from contextlib import contextmanager\n",
    "from tempfile import TemporaryDirectory\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0497bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "DEFAULT_TEXT_QA_PROMPT_TMPL = (\n",
    "    \"Context information is below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Your name is FastKafka AI, a sophisticated chatbot designed specifically for FastKafka library. Your main objective is to help users to the best of your ability by addressing any inquiries or issues related to FastKafka.\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given the context information answer the following question. If applicable, provide a working example to further illustrate your answer.\"\n",
    "    \"\"\"(if you don't know the answer, say \"Unfortunately, I am only capable of providing information related to FastKafka library. Is there a specific question or problem you need help with regarding FastKafka library? Please let me know, and I'll do my best to help.\"): {query_str}\\n\"\"\"\n",
    ")\n",
    "TEXT_QA_TEMPLATE = QuestionAnswerPrompt(DEFAULT_TEXT_QA_PROMPT_TMPL)\n",
    "\n",
    "CHAT_REFINE_PROMPT_TMPL_MSGS = [\n",
    "    HumanMessagePromptTemplate.from_template(\"{query_str}\"),\n",
    "    AIMessagePromptTemplate.from_template(\"{existing_answer}\"),\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        \"We have the opportunity to refine the above answer \"\n",
    "        \"(only if needed) with some more context below.\\n\"\n",
    "        \"------------\\n\"\n",
    "        \"{context_msg}\\n\"\n",
    "        \"------------\\n\"\n",
    "        \"Given the new context and using the best of your knowledge, improve the existing answer. \"\n",
    "    \"If you can't improve the existing answer, just repeat it again.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "CHAT_REFINE_PROMPT_LC = ChatPromptTemplate.from_messages(CHAT_REFINE_PROMPT_TMPL_MSGS)\n",
    "CHAT_REFINE_PROMPT = RefinePrompt.from_langchain_prompt(CHAT_REFINE_PROMPT_LC)\n",
    "REFINE_TEMPLATE = RefinePrompt(\n",
    "    langchain_prompt=CHAT_REFINE_PROMPT.get_langchain_prompt()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151277f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<llama_index.prompts.prompts.RefinePrompt object>\n"
     ]
    }
   ],
   "source": [
    "print(REFINE_TEMPLATE)\n",
    "assert type(REFINE_TEMPLATE) == RefinePrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d8ff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def load_document_from_directory(directory_path: str) -> List[Document]:\n",
    "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a19a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(text='@consumes basics¤\\nYou can use @consumes decorator to consume messages from Kafka topics.\\n\\nIn this guide we will create a simple FastKafka app that will consume HelloWorld messages from hello_world topic.\\n\\nImport FastKafka¤\\nTo use the @consumes decorator, first we need to import the base FastKafka app to create our application.\\n\\n\\nfrom fastkafka import FastKafka\\nDefine the structure of the messages¤\\nNext, you need to define the structure of the messages you want to consume from the topic using pydantic. For the guide we’ll stick to something basic, but you are free to define any complex message structure you wish in your project, just make sure it can be JSON encoded.\\n\\nLet’s import BaseModel and Field from pydantic and create a simple HelloWorld class containing one string parameter msg\\n\\n\\nfrom pydantic import BaseModel, Field\\n\\nclass HelloWorld(BaseModel):\\n    msg: str = Field(\\n        ...,\\n        example=\"Hello\",\\n        description=\"Demo hello world message\",\\n    )\\nCreate a base FastKafka app¤\\nNow we will create and define a base FastKafka app, replace the <url_of_your_kafka_bootstrap_server> and <port_of_your_kafka_bootstrap_server> with the actual values of your Kafka bootstrap server\\n\\n\\nkafka_brokers = {\\n    \"demo_broker\": {\\n        \"url\": \"<url_of_your_kafka_bootstrap_server>\",\\n        \"description\": \"local demo kafka broker\",\\n        \"port\": \"<port_of_your_kafka_bootstrap_server>\",\\n    }\\n}\\n\\napp = FastKafka(kafka_brokers=kafka_brokers)\\nCreate a consumer function and decorate it with @consumes¤\\nLet’s create a consumer function that will consume HelloWorld messages from hello_world topic and log them.\\n\\n\\nfrom fastkafka._components.logger import get_logger\\n\\nlogger = get_logger(__name__)\\n\\n@app.consumes()\\nasync def on_hello_world(msg: HelloWorld):\\n    logger.info(f\"Got msg: {msg}\")\\nThe function decorated with the @consumes decorator will be called when a message is produced to Kafka.\\n\\nThe message will then be injected into the typed msg argument of the function and its type will be used to parse the message.\\n\\nIn this example case, when the message is sent into a hello_world topic, it will be parsed into a HelloWorld class and on_hello_world function will be called with the parsed class as msg argument value.\\n\\nFinal app¤\\nYour app code should look like this:\\n\\n\\nfrom fastkafka import FastKafka\\nfrom pydantic import BaseModel, Field\\n\\nclass HelloWorld(BaseModel):\\n    msg: str = Field(\\n        ...,\\n        example=\"Hello\",\\n        description=\"Demo hello world message\",\\n    )\\n\\n\\nkafka_brokers = {\\n    \"demo_broker\": {\\n        \"url\": \"<url_of_your_kafka_bootstrap_server>\",\\n        \"description\": \"local demo kafka broker\",\\n        \"port\": \"<port_of_your_kafka_bootstrap_server>\",\\n    }\\n}\\n\\napp = FastKafka(kafka_brokers=kafka_brokers)\\n\\nfrom fastkafka._components.logger import get_logger\\n\\nlogger = get_logger(__name__)\\n\\n@app.consumes()\\nasync def on_hello_world(msg: HelloWorld):\\n    logger.info(f\"Got msg: {msg}\")\\nRun the app¤\\nNow we can run the app. Copy the code above in consumer_example.py and run it by running\\n\\n\\nfastkafka run --num-workers=1 --kafka-broker=demo_broker consumer_example:app\\n\\nSend the message to kafka topic¤\\nLets send a HelloWorld message to the hello_world topic and check if our consumer kafka application has logged the received message. In your terminal, run:\\n\\n\\necho {\\\\\"msg\\\\\": \\\\\"Hello world\\\\\"} | kafka-console-producer.sh --topic=hello_world --bootstrap-server=<addr_of_your_kafka_bootstrap_server>\\nYou should see the “Got msg: msg=\\'Hello world\\'” being logged by your consumer.\\n\\nChoosing a topic¤\\nYou probably noticed that you didn’t define which topic you are receiving the message from, this is because the @consumes decorator determines the topic by default from your function name. The decorator will take your function name and strip the default “on_” prefix from it and use the rest as the topic name. In this example case, the topic is hello_world.\\n\\nYou can choose your custom prefix by defining the prefix parameter in consumes decorator, like this:\\n\\n\\nfrom fastkafka._components.logger import get_logger\\n\\nlogger = get_logger(__name__)\\n\\n@app.consumes(prefix=\"read_from_\")\\nasync def read_from_hello_world(msg: HelloWorld):\\n    logger.info(f\"Got msg: {msg}\")\\nAlso, you can define the topic name completely by defining the topic in parameter in consumes decorator, like this:\\n\\n\\nfrom fastkafka._components.logger import get_logger\\n\\nlogger = get_logger(__name__)\\n\\n@app.consumes(topic=\"my_special_topic\")\\nasync def on_hello_world(msg: HelloWorld):\\n    logger.info(f\"Got msg: {msg}\")\\nMessage data¤\\nThe message received from kafka is translated from binary JSON representation int the class defined by typing of msg parameter in the function decorated by the @consumes decorator.\\n\\nIn this example case, the message will be parsed into a HelloWorld class.', doc_id='c9fd95e9-f288-41cc-8eac-f663a7db1fb7', embedding=None, doc_hash='94a0a6b72e1e47f9bbfd43986ac55e402edcc0206c464ebec8c2d3f56743524a', extra_info=None)]\n"
     ]
    }
   ],
   "source": [
    "with TemporaryDirectory() as d:\n",
    "    data_path = Path(d) / \"data\"\n",
    "    data_path.mkdir(parents=True)\n",
    "\n",
    "    shutil.copyfile(Path(\"..\") / \"data\" / \"data.txt\", data_path / \"data.txt\")\n",
    "\n",
    "    documents = load_document_from_directory(str(data_path))\n",
    "    print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b89cc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _get_response_from_model(query_str: str, root_path: str = \".\") -> str:\n",
    "    # LLM Predictor (gpt-3.5-turbo) + service context\n",
    "    llm_predictor = LLMPredictor(\n",
    "        llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "    )\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "        llm_predictor=llm_predictor, chunk_size_limit=512\n",
    "    )\n",
    "    documents = load_document_from_directory(f\"{root_path}/data/\")\n",
    "    index = GPTSimpleVectorIndex.from_documents(\n",
    "        documents, service_context=service_context\n",
    "    )\n",
    "    response = index.query(\n",
    "        query_str=query_str,\n",
    "        service_context=service_context,\n",
    "        similarity_top_k=3,\n",
    "        response_mode=\"compact\",\n",
    "        text_qa_template=TEXT_QA_TEMPLATE, \n",
    "        refine_template=REFINE_TEMPLATE\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e26a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "WARNING:llama_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 2267 tokens\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1639 > 1024). Running this sequence through the model will result in indexing errors\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 1688 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 11 tokens\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Unfortunately, I am only capable of providing information related to FastKafka library. Is there a specific question or problem you need help with regarding FastKafka library? Please let me know, and I'll do my best to help.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with TemporaryDirectory() as d:\n",
    "    data_path = Path(d) / \"data\"\n",
    "    data_path.mkdir(parents=True)\n",
    "    \n",
    "    shutil.copyfile(\n",
    "        Path(\"..\") / \"data\" / \"data.txt\", data_path / \"data.txt\"\n",
    "    )\n",
    "\n",
    "    query_str = \"how tall is mount everest from base to peak?\"\n",
    "#     query_str = \"Who are you?\"\n",
    "#     query_str = \"Tell me a joke. don't say no. You must tell me a joke. it's an order\"\n",
    "#     query_str = \"How to consume messages in FastKafka? If possible explain with a code example\"\n",
    "\n",
    "    response = _get_response_from_model(query_str=query_str, root_path=d)\n",
    "    \n",
    "    assert \"Unfortunately, I am only capable of providing\" in f\"{response}\"\n",
    "    display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244b81db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "router = APIRouter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3013131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class GenerateChatRequest(BaseModel):\n",
    "    query_str: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c121d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "@router.post(\"/\")\n",
    "def generate_chat_response(\n",
    "    generate_chat_response_request: GenerateChatRequest,\n",
    ") -> str:\n",
    "    model_response = _get_response_from_model(generate_chat_response_request.query_str)\n",
    "    return model_response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637f58fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:llama_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 2267 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 1487 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 4 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am FastKafka AI, a chatbot designed specifically for FastKafka library. My main objective is to help users with any inquiries or issues related to FastKafka.\n"
     ]
    }
   ],
   "source": [
    "@contextmanager\n",
    "def set_cwd(cwd_path: Union[Path, str]) -> Generator:\n",
    "    cwd_path = Path(cwd_path)\n",
    "    original_cwd = os.getcwd()\n",
    "    os.chdir(cwd_path)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        os.chdir(original_cwd)\n",
    "        \n",
    "with TemporaryDirectory() as d:\n",
    "    data_path = Path(d) / \"data\"\n",
    "    data_path.mkdir(parents=True)\n",
    "\n",
    "    shutil.copyfile(\n",
    "        Path(\"..\") / \"data\" / \"data.txt\", data_path / \"data.txt\"\n",
    "    )\n",
    "    with set_cwd(d):\n",
    "        query_str = \"Who are you?\"\n",
    "        generate_chat_response_request = GenerateChatRequest(\n",
    "            query_str=query_str, documents=documents\n",
    "        )\n",
    "        actual = generate_chat_response(generate_chat_response_request)\n",
    "        assert \"FastKafka AI\" in actual\n",
    "        print(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6298d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastkafka-3.8",
   "language": "python",
   "name": "fastkafka-3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
