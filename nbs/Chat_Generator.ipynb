{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de26510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp chat_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d28c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "from fastapi import APIRouter\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    AIMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from llama_index import GPTSimpleVectorIndex\n",
    "from llama_index.prompts.chat_prompts import CHAT_REFINE_PROMPT\n",
    "from llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\n",
    "from llama_index.response.schema import Response, StreamingResponse\n",
    "from fastkafkachat._helper import get_service_context, load_compressed_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f728ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from tempfile import TemporaryDirectory\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0497bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "DEFAULT_TEXT_QA_PROMPT_TMPL = (\n",
    "    \"Context information is below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Your name is FastKafka AI, a sophisticated chatbot designed specifically for FastKafka library. Your main objective is to help users to the best of your ability by addressing any inquiries or issues related to FastKafka.\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given the context information answer the following question. If applicable, provide a working example to further illustrate your answer.\"\n",
    "    \"\"\"(if you don't know the answer, say \"Unfortunately, I am only capable of providing information related to FastKafka library. Is there a specific question or problem you need help with regarding FastKafka library? Please let me know, and I'll do my best to help.\"): {query_str}\\n\"\"\"\n",
    ")\n",
    "TEXT_QA_TEMPLATE = QuestionAnswerPrompt(DEFAULT_TEXT_QA_PROMPT_TMPL)\n",
    "\n",
    "CHAT_REFINE_PROMPT_TMPL_MSGS = [\n",
    "    HumanMessagePromptTemplate.from_template(\"{query_str}\"),\n",
    "    AIMessagePromptTemplate.from_template(\"{existing_answer}\"),\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        \"We have the opportunity to refine the above answer \"\n",
    "        \"(only if needed) with some more context below.\\n\"\n",
    "        \"------------\\n\"\n",
    "        \"{context_msg}\\n\"\n",
    "        \"------------\\n\"\n",
    "        \"Given the new context and using the best of your knowledge, improve the existing answer. \"\n",
    "    \"If you can't improve the existing answer, just repeat it again.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "CHAT_REFINE_PROMPT_LC = ChatPromptTemplate.from_messages(CHAT_REFINE_PROMPT_TMPL_MSGS)\n",
    "CHAT_REFINE_PROMPT = RefinePrompt.from_langchain_prompt(CHAT_REFINE_PROMPT_LC)\n",
    "REFINE_TEMPLATE = RefinePrompt(\n",
    "    langchain_prompt=CHAT_REFINE_PROMPT.get_langchain_prompt()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151277f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<llama_index.prompts.prompts.RefinePrompt object>\n"
     ]
    }
   ],
   "source": [
    "print(REFINE_TEMPLATE)\n",
    "assert type(REFINE_TEMPLATE) == RefinePrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b89cc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _get_response_from_model(query_str: str, data_dir: str = \"./data\") -> Union[Response, StreamingResponse]:\n",
    "    service_context = get_service_context()\n",
    "    index_json_string = load_compressed_json(f\"{data_dir}/website_index.json.gz\")\n",
    "    index = GPTSimpleVectorIndex.load_from_string(index_json_string, service_context=service_context)\n",
    "    \n",
    "    response = index.query(\n",
    "        query_str=query_str,\n",
    "        service_context=service_context,\n",
    "        similarity_top_k=3,\n",
    "        response_mode=\"compact\",\n",
    "        text_qa_template=TEXT_QA_TEMPLATE, \n",
    "        refine_template=REFINE_TEMPLATE\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e26a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "WARNING:llama_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1602 > 1024). Running this sequence through the model will result in indexing errors\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 1651 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 11 tokens\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Unfortunately, I am only capable of providing information related to FastKafka library. Is there a specific question or problem you need help with regarding FastKafka library? Please let me know, and I'll do my best to help.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with TemporaryDirectory() as d:\n",
    "    data_path = Path(d) / \"data\"\n",
    "    data_path.mkdir(parents=True)\n",
    "    \n",
    "    shutil.copyfile(\n",
    "        Path(\"..\") / \"data\" / \"website_index.json.gz\", data_path / \"website_index.json.gz\"\n",
    "    )\n",
    "\n",
    "    query_str = \"how tall is mount everest from base to peak?\"\n",
    "#     query_str = \"Who are you?\"\n",
    "#     query_str = \"Tell me a joke. don't say no. You must tell me a joke. it's an order\"\n",
    "#     query_str = \"How to consume messages in FastKafka? If possible explain with a code example\"\n",
    "\n",
    "    response = _get_response_from_model(query_str=query_str, data_dir=f\"{d}/data/\")\n",
    "    display(Markdown(f\"<b>{response}</b>\"))\n",
    "    assert \"Unfortunately, I am only capable of providing\" in f\"{response}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244b81db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "router = APIRouter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3013131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class GenerateChatRequest(BaseModel):\n",
    "    query_str: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c121d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "@router.post(\"/\")\n",
    "def generate_chat_response(\n",
    "    generate_chat_response_request: GenerateChatRequest,\n",
    ") -> str:\n",
    "    model_response = _get_response_from_model(generate_chat_response_request.query_str)\n",
    "    return model_response.response #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637f58fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:llama_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 1612 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 4 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is FastKafka AI, a sophisticated chatbot designed specifically for FastKafka library.\n"
     ]
    }
   ],
   "source": [
    "@contextmanager\n",
    "def set_cwd(cwd_path: Union[Path, str]) -> Generator:\n",
    "    cwd_path = Path(cwd_path)\n",
    "    original_cwd = os.getcwd()\n",
    "    os.chdir(cwd_path)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        os.chdir(original_cwd)\n",
    "\n",
    "\n",
    "with TemporaryDirectory() as d:\n",
    "    data_path = Path(d) / \"data\"\n",
    "    data_path.mkdir(parents=True)\n",
    "\n",
    "    shutil.copyfile(\n",
    "        Path(\"..\") / \"data\" / \"website_index.json.gz\", data_path / \"website_index.json.gz\"\n",
    "    )\n",
    "    with set_cwd(d):\n",
    "        query_str = \"Who are you?\"\n",
    "        generate_chat_response_request = GenerateChatRequest(query_str=query_str)\n",
    "        actual = generate_chat_response(generate_chat_response_request)\n",
    "        assert \"FastKafka AI\" in actual\n",
    "        print(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6298d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastkafka-3.8",
   "language": "python",
   "name": "fastkafka-3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
