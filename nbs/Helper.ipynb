{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679d8511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp _helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0c234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "import gzip\n",
    "from urllib.request import Request, urlopen\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from llama_index import (\n",
    "    GPTSimpleVectorIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    LLMPredictor,\n",
    "    ServiceContext,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dc338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import TemporaryDirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c460468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def get_all_links_from_website(start_url: str, visited: Optional[set] = None) -> Set[str]:\n",
    "    \"\"\"Get a set of all links (URLs) found on the given website, starting from the given start URL.\n",
    "    \n",
    "    Args:\n",
    "        start_url: The starting URL of the website.\n",
    "        visited: Optional. A set of URLs that have already been visited. Defaults to an empty set.\n",
    "\n",
    "    Returns:\n",
    "        A set of all links found on the website.\n",
    "    \"\"\"\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "\n",
    "    req = Request(start_url)\n",
    "    html_page = urlopen(req)\n",
    "    soup = BeautifulSoup(html_page, \"lxml\")\n",
    "    \n",
    "    base_url = urlparse(start_url).scheme + '://' + urlparse(start_url).hostname #type: ignore\n",
    "    \n",
    "    links = set()\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        url = urljoin(base_url, link['href']).split(\"#\")[0].strip(\"/\")\n",
    "        if urlparse(url).hostname == urlparse(start_url).hostname:\n",
    "            links.add(url)\n",
    "            \n",
    "    visited.add(start_url)\n",
    "    for link in links:\n",
    "        if link not in visited:\n",
    "            visited |= get_all_links_from_website(link, visited)\n",
    "    \n",
    "    return visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7336e725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'https://fastkafka.airt.ai/docs', 'https://fastkafka.airt.ai/docs/cli/fastkafka', 'https://fastkafka.airt.ai/docs/guides/Guide_04_Github_Actions_Workflow', 'https://fastkafka.airt.ai/docs/guides/Guide_11_Consumes_Basics', 'https://fastkafka.airt.ai/docs/guides/Guide_30_Using_docker_to_deploy_fastkafka', 'https://fastkafka.airt.ai/docs/api/fastkafka', 'https://fastkafka.airt.ai/docs/CHANGELOG', 'https://fastkafka.airt.ai/docs/cli/run_fastkafka_server_process', 'https://fastkafka.airt.ai/docs/guides/Guide_21_Produces_Basics', 'https://fastkafka.airt.ai/docs/guides/Guide_31_Using_redpanda_to_test_fastkafka', 'https://fastkafka.airt.ai', 'https://fastkafka.airt.ai/docs/api/fastkafka/KafkaEvent', 'https://fastkafka.airt.ai/docs/api/fastkafka/testing/LocalRedpandaBroker', 'https://fastkafka.airt.ai/docs/api/fastkafka/testing/ApacheKafkaBroker', 'https://fastkafka.airt.ai/docs/api/fastkafka/testing/Tester', 'https://fastkafka.airt.ai/docs/guides/Guide_22_Partition_Keys'}\n",
      "\n",
      "\n",
      "len(all_links)=16\n"
     ]
    }
   ],
   "source": [
    "all_links = get_all_links_from_website(\"https://fastkafka.airt.ai\")\n",
    "print(all_links)\n",
    "print(f\"\\n\\n{len(all_links)=}\")\n",
    "\n",
    "assert len(all_links) > 0\n",
    "assert 'https://fastkafka.airt.ai/docs/CHANGELOG' in all_links\n",
    "assert 'https://fastkafka.airt.ai/docs' in all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff74fdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def get_service_context() -> ServiceContext:\n",
    "    \"\"\"Return a service context object initialized with an LLM predictor based on the gpt-3.5-turbo model\n",
    "    \n",
    "    Returns:\n",
    "        A ServiceContext object with an LLMPredictor and a chunk size limit.\n",
    "    \"\"\"\n",
    "    llm_predictor = LLMPredictor(\n",
    "        llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "    )\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "        llm_predictor=llm_predictor, chunk_size_limit=512\n",
    "    )\n",
    "    \n",
    "    return service_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6b431a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "WARNING:llama_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ServiceContext(llm_predictor=<llama_index.llm_predictor.base.LLMPredictor object>, prompt_helper=<llama_index.indices.prompt_helper.PromptHelper object>, embed_model=<llama_index.embeddings.openai.OpenAIEmbedding object>, node_parser=<llama_index.node_parser.simple.SimpleNodeParser object>, llama_logger=<llama_index.logger.base.LlamaLogger object>, chunk_size_limit=512)\n"
     ]
    }
   ],
   "source": [
    "service_context = get_service_context()\n",
    "\n",
    "print(service_context)\n",
    "assert type(service_context) == ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8438261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def write_compressed_json(json_string: str, file_path: str) -> None:\n",
    "    \"\"\"Compresses a JSON string and writes it to disk at the specified file path with a .gz extension.\n",
    "    \n",
    "    Args:\n",
    "        json_string: The JSON string to compress and write to disk.\n",
    "        file_path: The path to write the compressed JSON file to, without the .gz extension.\n",
    "    \"\"\"\n",
    "    with gzip.open(file_path + '.gz', 'wb') as f_out:\n",
    "        json_bytes = json_string.encode('utf-8')\n",
    "        f_out.write(json_bytes)\n",
    "        \n",
    "        \n",
    "def load_compressed_json(gziped_json_file_path: str) -> str:\n",
    "    \"\"\"Load a compressed JSON file from disk and returns its contents as a string.\n",
    "    \n",
    "    Args:\n",
    "        gziped_json_file_path: The path to the compressed JSON file to read.\n",
    "    \"\"\"\n",
    "    with gzip.open(gziped_json_file_path, 'rb') as f:\n",
    "        json_bytes = f.read()\n",
    "        json_str = json_bytes.decode('utf-8')\n",
    "        return json_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3474178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \"name\":\"John\", \"age\":30, \"city\":\"New York\"}\n"
     ]
    }
   ],
   "source": [
    "with TemporaryDirectory() as d:\n",
    "    data_path = Path(d) / \"data\"\n",
    "    data_path.mkdir(parents=True)\n",
    "    \n",
    "    expected =  '{ \"name\":\"John\", \"age\":30, \"city\":\"New York\"}'\n",
    "    file_path=f\"{d}/data/website_index.json\"\n",
    "    \n",
    "    write_compressed_json(expected, file_path)\n",
    "    assert Path(file_path + '.gz').exists()\n",
    "    \n",
    "    actual = load_compressed_json(file_path + '.gz')\n",
    "    print(actual)\n",
    "    \n",
    "    assert actual == expected"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastkafka-3.8)",
   "language": "python",
   "name": "fastkafka-3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
