# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/Chat_Generator.ipynb.

# %% auto 0
__all__ = ['DEFAULT_TEXT_QA_PROMPT_TMPL', 'TEXT_QA_TEMPLATE', 'CHAT_REFINE_PROMPT_TMPL_MSGS', 'CHAT_REFINE_PROMPT_LC',
           'CHAT_REFINE_PROMPT', 'REFINE_TEMPLATE', 'router', 'GenerateChatRequest', 'generate_chat_response']

# %% ../nbs/Chat_Generator.ipynb 1
from pathlib import Path
from typing import *

from fastapi import APIRouter
from pydantic import BaseModel


from langchain.prompts.chat import (
    AIMessagePromptTemplate,
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
)
from llama_index import GPTVectorStoreIndex, StorageContext, load_index_from_storage
from llama_index.prompts.chat_prompts import CHAT_REFINE_PROMPT
from llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt
from llama_index.response.schema import Response, StreamingResponse
from ._helper import get_service_context, unzip_index_files

# %% ../nbs/Chat_Generator.ipynb 3
DEFAULT_TEXT_QA_PROMPT_TMPL = (
    "Context information is below. \n"
    "---------------------\n"
    "Your name is FastKafka AI, a sophisticated chatbot designed specifically for FastKafka library. Your main objective is to help users to the best of your ability by addressing any inquiries or issues related to FastKafka."
    "\n---------------------\n"
    "---------------------\n"
    "{context_str}"
    "\n---------------------\n"
    "Given the context information answer the following question. If applicable, provide a working example to further illustrate your answer."
    """(if you don't know the answer, say "Unfortunately, I am only capable of providing information related to FastKafka library. Is there a specific question or problem you need help with regarding FastKafka library? Please let me know, and I'll do my best to help."): {query_str}\n"""
)
TEXT_QA_TEMPLATE = QuestionAnswerPrompt(DEFAULT_TEXT_QA_PROMPT_TMPL)

CHAT_REFINE_PROMPT_TMPL_MSGS = [
    HumanMessagePromptTemplate.from_template("{query_str}"),
    AIMessagePromptTemplate.from_template("{existing_answer}"),
    HumanMessagePromptTemplate.from_template(
        "We have the opportunity to refine the above answer "
        "(only if needed) with some more context below.\n"
        "------------\n"
        "{context_msg}\n"
        "------------\n"
        "Given the new context and using the best of your knowledge, improve the existing answer. "
    "If you can't improve the existing answer, just repeat it again."
    ),
]

CHAT_REFINE_PROMPT_LC = ChatPromptTemplate.from_messages(CHAT_REFINE_PROMPT_TMPL_MSGS)
CHAT_REFINE_PROMPT = RefinePrompt.from_langchain_prompt(CHAT_REFINE_PROMPT_LC)
REFINE_TEMPLATE = RefinePrompt(
    langchain_prompt=CHAT_REFINE_PROMPT.get_langchain_prompt()
)

# %% ../nbs/Chat_Generator.ipynb 5
def _get_response_from_model(
    query_str: str, data_dir: str = "./data"
) -> Union[Response, StreamingResponse]:
    service_context = get_service_context()
    if not all(
        [
            Path(f"{data_dir}/{file}").exists()
            for file in ["docstore.json", "index_store.json", "vector_store.json"]
        ]
    ):
        unzip_index_files(f"{data_dir}/website_index.zip")

    storage_context = StorageContext.from_defaults(persist_dir=data_dir)
    index = load_index_from_storage(storage_context, service_context=service_context)
    query_engine = index.as_query_engine(
        service_context=service_context,
        similarity_top_k=3,
        response_mode="compact",
        text_qa_template=TEXT_QA_TEMPLATE,
        refine_template=REFINE_TEMPLATE,
    )

    response = query_engine.query(query_str)
    return response

# %% ../nbs/Chat_Generator.ipynb 8
router = APIRouter()


# %% ../nbs/Chat_Generator.ipynb 9
class GenerateChatRequest(BaseModel):
    query_str: str

# %% ../nbs/Chat_Generator.ipynb 10
@router.post("/")
def generate_chat_response(
    generate_chat_response_request: GenerateChatRequest,
) -> str:
    model_response = _get_response_from_model(generate_chat_response_request.query_str)
    return model_response.response #type: ignore
