# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/Chat_Generator.ipynb.

# %% auto 0
__all__ = ['SYSTEM_INSTRUCTION', 'DEFAULT_MESSAGE_TEMPLATE', 'router', 'GenerateChatRequest', 'generate_chat_response']

# %% ../nbs/Chat_Generator.ipynb 1
from pathlib import Path
from typing import *
from os import environ
import random
import logging
import time

from fastapi import APIRouter
from pydantic import BaseModel
import openai

# %% ../nbs/Chat_Generator.ipynb 3
# Reference: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb


def _retry_with_exponential_backoff(
    initial_delay: float = 1,
    exponential_base: float = 2,
    jitter: bool = True,
    max_retries: int = 10,
    max_wait: float = 60,
    errors: tuple = (
        openai.error.RateLimitError,
        openai.error.ServiceUnavailableError,
        openai.error.APIError,
    ),
) -> Callable:
    """Retry a function with exponential backoff."""

    def decorator(func):
        def wrapper(*args, **kwargs):
            num_retries = 0
            delay = initial_delay

            while True:
                try:
                    return func(*args, **kwargs)

                except errors as e:
                    num_retries += 1
                    if num_retries > max_retries:
                        raise Exception(
                            f"Maximum number of retries ({max_retries}) exceeded."
                        )
                    delay = min(
                        delay
                        * exponential_base
                        * (1 + jitter * random.random()),  # nosec
                        max_wait,
                    )
                    logging.info(
                        f"Note: OpenAI's API rate limit reached. Command will automatically retry in {int(delay)} seconds. For more information visit: https://help.openai.com/en/articles/5955598-is-api-usage-subject-to-any-rate-limits",
                    )
                    time.sleep(delay)

                except Exception as e:
                    raise e

        return wrapper

    return decorator


@_retry_with_exponential_backoff()
def _completions_with_backoff(*args, **kwargs):
    return openai.ChatCompletion.create(*args, **kwargs)

# %% ../nbs/Chat_Generator.ipynb 4
SYSTEM_INSTRUCTION = {
    "role": "system",
    "content": """Your name is Fastkafka AI, an advanced chatbot specialized in Fastkafka. Your primary goal is to assist users to the best of your ability. 
""",
}

# %% ../nbs/Chat_Generator.ipynb 6
DEFAULT_MESSAGE_TEMPLATE = [SYSTEM_INSTRUCTION] #+ FEW_SHOT_EXAMPLES

# %% ../nbs/Chat_Generator.ipynb 12
router = APIRouter()


# %% ../nbs/Chat_Generator.ipynb 13
class GenerateChatRequest(BaseModel):
    message_history: List[Dict[str, str]]



# %% ../nbs/Chat_Generator.ipynb 14
@router.post("/")
def generate_chat_response(
    generate_chat_response_request: GenerateChatRequest,
) -> Dict[str, str]:
    """Generate a chat response.

    Args:
        generate_chat_response_request: A GenerateChatRequest object.

    Returns:
        The response generated by Open Ai's text-davinci-003 model

    !!! note

        The above docstring is autogenerated by docstring-gen library (https://docstring-gen.airt.ai)
    """
    messages = DEFAULT_MESSAGE_TEMPLATE + generate_chat_response_request.message_history
    response = _completions_with_backoff(
        model="gpt-3.5-turbo",
        messages=messages,
        temperature=0
    )
    ret_val = {"message": response['choices'][0]['message']['content']}
    return ret_val
